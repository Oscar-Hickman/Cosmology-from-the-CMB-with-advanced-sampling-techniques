{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#EXCLUDE_FROM_PY\n",
        "\n",
        "\"\"\"\n",
        "Generate a .py file with all callable methods from the .ipynb file which can saved in gdrive, excludes cells with EXCLUDE_FROM_PY\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import nbformat\n",
        "\n",
        "#Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#Define the target folder in your Drive\n",
        "folder_path = \"/content/drive/MyDrive/CMB_with_advanced_sampling_techniques\"\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "#Navigate to the correct directory (where your notebook is saved)\n",
        "os.chdir(folder_path)  #Change directory to MyDrive\n",
        "print(\"Current Directory:\", os.getcwd())  #Debugging step\n",
        "\n",
        "# Read the notebook and extract code cells with function and class definitions (exclude EXCLUDE_FROM_PY cells)\n",
        "notebook_path = \"/content/drive/MyDrive/CMB_with_advanced_sampling_techniques/Main.ipynb\"\n",
        "with open(notebook_path) as f:\n",
        "    notebook_content = nbformat.read(f, as_version=4)\n",
        "\n",
        "#Collect only code cells with function and class definitions, and not marked for exclusion\n",
        "code_cells = []\n",
        "for cell in notebook_content.cells:\n",
        "    if cell.cell_type == 'code' and \"#EXCLUDE_FROM_PY\" not in cell.source:\n",
        "        code = cell.source\n",
        "        if \"def \" in code or \"class \" in code:  # Only include function/class definitions\n",
        "            code_cells.append(code)\n",
        "\n",
        "#Combine all code into a single Python script\n",
        "script_content = \"\\n\\n\".join(code_cells)\n",
        "\n",
        "#Convert .ipynb to .py\n",
        "main_py_path = os.path.join(folder_path, \"Main.py\")\n",
        "with open(main_py_path, \"w\") as f:\n",
        "    f.write(script_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6BoHpIZ4qmbv",
        "outputId": "0808bcc5-cecc-4461-8fc2-953e8884d9a7"
      },
      "outputs": [],
      "source": [
        "#EXCLUDE_FROM_PY\n",
        "\n",
        "\"\"\"\n",
        "Import Packages\n",
        "\"\"\"\n",
        "!pip install -q healpy\n",
        "!pip install camb\n",
        "!pip install corner\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import healpy as hp\n",
        "import camb\n",
        "from camb import model, initialpower\n",
        "import glob\n",
        "import pylab as plty\n",
        "from PIL import Image\n",
        "from healpy.sphtfunc import Alm\n",
        "import time\n",
        "import corner\n",
        "import scipy.stats as st\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv1_93_QqRPG"
      },
      "outputs": [],
      "source": [
        "#Use CAMB to generate a power spectrum\n",
        "def call_CAMB_map(_parameters, _lmax):\n",
        "    \"\"\"\n",
        "    Use CAMB to generate a power spectrum.\n",
        "\n",
        "     Parameters:\n",
        "    _parameters (list): List of cosmological parameters [H0, ombh2, omch2, mnu, omk, tau].\n",
        "                        H0: Hubble Constant\n",
        "                        ombh2: Baryon density\n",
        "                        omch2: Dark Matter density\n",
        "                        mnu: Sum of 3 neutrino masses in eV\n",
        "                        omk: Curvature parameter (Omega kappa)\n",
        "                        tau: Reionisation optical depth\n",
        "    _lmax (int): Maximum multipole moment. \n",
        "    \n",
        "    Returns:\n",
        "    numpy.ndarray: Power spectrum (C_l) up to the given _lmax.\n",
        "    \"\"\"\n",
        "    if _lmax <= 2551: #can only find power spectrum for lmax <= 2551 since that is the maximum value of the data.\n",
        "        pars = camb.CAMBparams()\n",
        "        pars.set_cosmology(H0 = _parameters[0], ombh2 = _parameters[1], omch2 = _parameters[2], mnu = _parameters[3],\n",
        "                   omk = _parameters[4], tau = _parameters[5])  #Inputs the given cosmological parameters.\n",
        "        pars.InitPower.set_params(As=2e-9, ns=0.965, r=0)\n",
        "\n",
        "        pars.set_for_lmax(_lmax, lens_potential_accuracy=0) #input the given lmax value\n",
        "\n",
        "        results = camb.get_results(pars)\n",
        "        powers = results.get_cmb_power_spectra(pars, CMB_unit='muK') #returns the power spectrum in units muK.\n",
        "\n",
        "        totCL=powers['total'] #returns the total (averaged) power spectrum - including lensed, unlensed power spectra\n",
        "        _DL = totCL[:,0]\n",
        "\n",
        "        #unlensedCL=powers['unlensed_scalar'] #returns the unlensed scalar power spectrum\n",
        "\n",
        "        _l = np.arange(len(_DL)) #not sure this CL is actually CL but is actually DL\n",
        "        _CL = []\n",
        "        for i in range(_lmax): #also limits the length of power spectrum to the requested length\n",
        "            if i == 0:\n",
        "                _CL.append(_DL[i]) #since unsure what CL value is for this DL\n",
        "\n",
        "            else:\n",
        "                _CL.append(_DL[i]/(_l[i]*(_l[i] + 1)))\n",
        "\n",
        "        _CL = np.array(_CL)\n",
        "\n",
        "        return _CL\n",
        "\n",
        "    else:\n",
        "        print('lmax value is larger than the available data.')\n",
        "\n",
        "#Plots a given power spectrum\n",
        "def plotpwrspctrm(_cls):\n",
        "    _l = np.arange(len(_cls))\n",
        "    plt.plot(_l, _l * (_l + 1) * _cls)\n",
        "    plt.xlabel(\"$\\l$\")\n",
        "    plt.ylabel(\"$\\l(\\l+1)C_{\\l}$\")\n",
        "    plt.grid()\n",
        "    plt.title(\"Power Spectrum\")\n",
        "\n",
        "#Plots a map in the mollview projection\n",
        "def mollviewmap(_map):\n",
        "    hp.mollview(_map, title=\"Map displayed in the Molleview projection\", cmap = None)\n",
        "    hp.graticule()\n",
        "\n",
        "#Adds random noise to each pixel on a map given a variance\n",
        "def noisemapfunc(_map,_var):\n",
        "    _noisevec = np.random.normal(0,_var, len(_map)) #A vector of the noise applied to each pixel\n",
        "    _newmap = [x + y for x, y in zip(_map, _noisevec)]\n",
        "    _newmap, _noisevec = np.array(_newmap), np.array(_noisevec)\n",
        "    return [_newmap, _noisevec] #returns an array consisiting of [map with added noise, array of the added noise]\n",
        "\n",
        "#cls --> something\n",
        "def cltoalm(_cls, _NSIDE, _lmax): #doesn't work (isnt currently being used)\n",
        "    _alms = []\n",
        "    _count = 0\n",
        "    for l in range(_lmax):\n",
        "        if _cls[l] > 0:\n",
        "            _alms.append(np.complex(np.random.normal(0,_cls[l]),0))        #set m=0, which is real\n",
        "        else:\n",
        "            _alms.append(np.complex(0,0))\n",
        "\n",
        "        for m in range(l+1): #set positive m's\n",
        "            if _cls[l] > 0 and _cls[m] > 0:\n",
        "                _alms.append(np.complex(np.random.normal(0,0.5*_cls[l]),np.random.normal(0,0.5*_cls[m])))\n",
        "            if _cls[l] > 0 and _cls[m] <= 0:\n",
        "                _alms.append(np.complex(np.random.normal(0,0.5*_cls[l]),0))\n",
        "            if _cls[l] <= 0 and _cls[m] > 0:\n",
        "                _alms.append(np.complex(0,np.random.normal(0,0.5*_cls[m])))\n",
        "            else:\n",
        "                _alms.append(np.complex(0,0))\n",
        "    return _alms\n",
        "\n",
        "#Healpy generate alms given cls\n",
        "def hpcltoalm(_cls, _NSIDE, _lmax):\n",
        "    return hp.synalm(_cls, _lmax - 1, new = True)\n",
        "\n",
        "#doesn't work (isnt currently being used)\n",
        "def cltomap(_cls, _NSIDE, _lmax):\n",
        "    _alm = cltoalm(_cls, _NSIDE, _lmax)\n",
        "    return almtomap(_alm, _NSIDE, _lmax)\n",
        "\n",
        "#Healpy generate a map given a power spectrum\n",
        "def hpcltomap(_cls, _NSIDE, _lmax):\n",
        "    return hp.synfast(_cls, _NSIDE, _lmax - 1, new=True)\n",
        "\n",
        "#Generate a power spectrum given cls\n",
        "def hpmaptocl(_map, _NSIDE, _lmax):\n",
        "    return hp.anafast(_map, lmax = _lmax - 1)    #lmax = 3NSIDE by default\n",
        "\n",
        "#does this manually - doesn't work (isnt currently being used)\n",
        "def maptoalm(_map):\n",
        "    _omegp = (4*np.pi)/len(_map)\n",
        "    _lmax = int(np.sqrt(len(_map)*(3/4)))\n",
        "    _NSIDE = int(_lmax/3)\n",
        "    _alm = []\n",
        "    for l in range(_lmax):\n",
        "        for m in range(l+1):\n",
        "            _TpYlm = []\n",
        "            for i in range(len(_map)):\n",
        "                _TpYlm.append(_map[i]*np.conjugate(sphharm(m, l, i, _NSIDE)))\n",
        "\n",
        "            _alm.append(_omegp*sum(_TpYlm))\n",
        "\n",
        "    return np.array(_alm)\n",
        "\n",
        "#Healpy generate alms from map.\n",
        "def hpmaptoalm(_map, _lmax):\n",
        "    return hp.map2alm(_map, _lmax-1)\n",
        "\n",
        "##alm --> something\n",
        "def almtocl(_alm, lmax): #alm --> cl using alms in my ordering (different to healpys).\n",
        "    _l = np.arange(lmax)\n",
        "    _scaling = 1 / ((2*_l + 1))\n",
        "    count = 0\n",
        "    _new = []\n",
        "    _cl = []\n",
        "    for l in range(lmax):\n",
        "        _new.append([])\n",
        "        for m in range(l):\n",
        "            if m == 0:\n",
        "                _new[l].append(np.absolute(_alm[count])**2)\n",
        "                count = count + 1\n",
        "\n",
        "            if m > 0:\n",
        "                _new[l].append(2*np.absolute(_alm[count])**2)\n",
        "                count = count + 1\n",
        "\n",
        "    for i in range(len(_new)):\n",
        "        _cl.append(_scaling[i] * sum(_new[i]))\n",
        "\n",
        "    return _cl\n",
        "\n",
        "#Healpy estimates the power spectrum from the cls.\n",
        "def hpalmtocl(_alms, _lmax):\n",
        "    return hp.alm2cl(_alms, lmax = _lmax-1)\n",
        "\n",
        "# alm --> map using alms in my ordering (different to healpys).    #used in psi\n",
        "def almtomap(_alm, _NSIDE, _lmax):\n",
        "    _map = []\n",
        "    _Npix = 12*(_NSIDE)**2\n",
        "\n",
        "    for i in range(_Npix):\n",
        "        _sum = []\n",
        "        _count = 0\n",
        "        for l in np.arange(0,_lmax):\n",
        "            for m in np.arange(0,l+1):\n",
        "                if m == 0:\n",
        "                    _sum.append(_alm[_count]*sphharm(m,l,i, _NSIDE))\n",
        "                    _count = _count + 1\n",
        "                else:\n",
        "                    _sum.append(2*(np.real(_alm[_count])*np.real(sphharm(m,l,i, _NSIDE)) -\n",
        "                                   np.imag(_alm[_count])*np.imag(sphharm(m,l,i, _NSIDE))))\n",
        "                    _count = _count + 1\n",
        "        _map.append(sum(_sum))\n",
        "\n",
        "    return np.real(_map)\n",
        "\n",
        "def almtomap_tf(_alm,_NSIDE, _lmax):  #used in psitf\n",
        "    _ones = np.ones(len(_alm), dtype = np.complex128)\n",
        "    _count = 0\n",
        "    for l in range(_lmax):\n",
        "        for m in range(l+1):\n",
        "            if m == 0:\n",
        "                _ones[_count] = np.complex(0.5,0)\n",
        "            _count = _count + 1\n",
        "    _ones = tf.convert_to_tensor(_ones)\n",
        "    _alm = _ones*_alm\n",
        "    _ralm = tf.math.real(_alm)\n",
        "    _ialm = tf.math.imag(_alm)\n",
        "    _rsph = tf.math.real(_sph)\n",
        "    _isph = tf.math.imag(_sph)\n",
        "\n",
        "    _map1 = tf.linalg.matvec(_rsph,_ralm)\n",
        "    _map2 = tf.linalg.matvec(_isph,_ialm)\n",
        "    _map = 2*(_map1 - _map2)\n",
        "    return _map\n",
        "\n",
        "def almtomap_tf2(_alm,_NSIDE, _lmax):\n",
        "    _map = tf.Variable([])\n",
        "    _ralm = tf.math.real(_alm)\n",
        "    _ialm = tf.math.imag(_alm)\n",
        "    _rsph = tf.math.real(_sph)\n",
        "    _isph = tf.math.imag(_sph)\n",
        "    _map = tf.Variable(np.array([]))\n",
        "    for i in range(12*(_NSIDE)**2):\n",
        "        _count = 0\n",
        "        _term1 = tf.Variable(0.0,dtype = np.float64)\n",
        "        for l in range(_lmax):\n",
        "            for m in range(l+1):\n",
        "                if m==0:\n",
        "                    tf.compat.v1.assign_add(_term1, _ralm[_count]*_rsph[i][_count])\n",
        "                    _count = _count + 1\n",
        "                else:\n",
        "                    tf.compat.v1.assign_add(_term1,2*(_ralm[_count]*_rsph[i][_count] -\n",
        "                                                                  _ialm[_count]*_isph[i][_count]),0.0)\n",
        "                    _count = _count + 1\n",
        "\n",
        "        _map = tf.concat((_map, [_term1]), axis = 0)\n",
        "    _map = tf.dtypes.cast(_map, np.float64)\n",
        "    return _map\n",
        "\n",
        "#healpy map build\n",
        "def hpalmtomap(_alms, _NSIDE, _lmax):\n",
        "    return hp.alm2map(_alms, _NSIDE ,_lmax-1)\n",
        "\n",
        "#healpy smoothing for the map and the alms\n",
        "def hpmapsmooth(_map, _lmax): #smooths a given map with a gaussian beam smoother.\n",
        "    return hp.smoothing(_map, lmax = _lmax)\n",
        "\n",
        "#healpy gaussian smoothing for the alms\n",
        "def hpalmsmooth(_alms): #smooths a given set of alms with a gaussian beam smoother.\n",
        "    return hp.smoothalm(_alms, fwhm = 0.0)\n",
        "\n",
        "#splits/rejoins the alms into real/imaginary parts so that they can be optimised with scipy.optimize.minimize()\n",
        "def singulartosplitalm(_alm):\n",
        "    _realalm, _imagalm = _alm.real, _alm.imag\n",
        "    return [_realalm, _imagalm]\n",
        "\n",
        "def splittosingularalm(_realalm, _imagalm):\n",
        "    _alm = []\n",
        "    _ralmcount = 0\n",
        "    _ialmcount = 0\n",
        "    for l in range(lmax):\n",
        "        for m in range(l+1):\n",
        "            if m == 0 or m == 1:\n",
        "                _alm.append(complex(_realalm[_ralmcount], 0))\n",
        "                _ralmcount = _ralmcount + 1\n",
        "            else:\n",
        "                _alm.append(complex(_realalm[_ralmcount], _imagalm[_ialmcount]))\n",
        "                _ralmcount = _ralmcount + 1\n",
        "                _ialmcount = _ialmcount + 1\n",
        "\n",
        "    return _alm\n",
        "\n",
        "\n",
        "def splittosingularalm_tf(_realalm, _imagalm): #takes the real and imaginary parts of the alms and creates a tensor\n",
        "    _zero = tf.zeros(1, dtype = np.float64)\n",
        "    _count = 0\n",
        "    for l in range(lmax): #pads zeros to to lmax = 0 values\n",
        "        for m in range(l + 1):\n",
        "            if m == 0 or m == 1:\n",
        "                if l == 0:\n",
        "                    _imagalm = tf.concat([_zero,_imagalm], axis = 0)\n",
        "                else:\n",
        "                    _front = _imagalm[:_count]\n",
        "                    _back = _imagalm[_count:]\n",
        "                    _term = tf.concat([_zero, _back] , axis = 0)\n",
        "                    _imagalm = tf.concat([_front, _term], axis = 0)\n",
        "            _count = _count + 1\n",
        "    return tf.complex(_realalm,_imagalm)\n",
        "\n",
        "#Retrieves the spherical harmonics for a given, l, m and pixel number\n",
        "def sphharm(m, l, _pixno, _NSIDE):\n",
        "    _theta, _phi = hp.pix2ang(nside=_NSIDE, ipix=_pixno)\n",
        "    return sp.special.sph_harm(m, l, _phi, _theta)\n",
        "\n",
        "#Changes the ordering of the alms from healpy to mine or vice versa\n",
        "def almmotho(_moalm, _lmax):\n",
        "    '''changing the alm ordering from my ordering to healpys'''\n",
        "    _hoalm = []\n",
        "    _count4 = []\n",
        "    _count5 = 0\n",
        "    for i in np.arange(2,_lmax+2):\n",
        "        _count4.append(_count5)\n",
        "        _count5=_count5+i\n",
        "    for i in range(_lmax):\n",
        "        _count1 = 0\n",
        "        _count2 = np.arange(0,_lmax,1)\n",
        "        _count3 = np.arange(_lmax,0,-1)\n",
        "        for j in np.arange(i+1,_lmax+1):\n",
        "            _hoalm.append(_moalm[_count1+_count4[i]])\n",
        "            _count1 = _count1 + j\n",
        "    return np.array(_hoalm)\n",
        "\n",
        "\n",
        "def almhotmo(_hoalm, _lmax):\n",
        "    '''changing the alm ordering from healpys ordering to mine'''\n",
        "    _moalm = np.zeros(sum(np.arange(_lmax+1)), dtype = complex)\n",
        "    _count4 = []\n",
        "    _count5 = 0\n",
        "    for i in np.arange(2,_lmax+2):\n",
        "        _count4.append(_count5)\n",
        "        _count5 = _count5+i\n",
        "    _count1 = 0\n",
        "    for i in range(_lmax):\n",
        "        _count2 = 0\n",
        "        for j in np.arange(i+1,_lmax+1):\n",
        "            _moalm[_count2 + _count4[i]] = _hoalm[_count1]\n",
        "            _count1 = _count1 + 1\n",
        "            _count2 = _count2 + j\n",
        "    return np.array(_moalm)\n",
        "\n",
        "#pads zeros to the real l=0 and l=1 terms of the alms - in my ordering\n",
        "def alminit(_alms, _lmax):\n",
        "    _count = 0\n",
        "    for l in range(_lmax):\n",
        "        for m in range(l + 1):\n",
        "            if l == 0 or l == 1:\n",
        "                _alms[_count] = complex(0,0)\n",
        "                _count = _count + 1\n",
        "    _count = 0\n",
        "    for l in range(_lmax):\n",
        "        for m in range(l + 1):\n",
        "            if m == 0 or m == 1:\n",
        "                _alms[_count] = complex(np.real(_alms[_count]),0)\n",
        "                _count = _count + 1\n",
        "            else:\n",
        "                _count = _count + 1\n",
        "    return _alms\n",
        "\n",
        "#pads zeros to the real l=0 and l=1 terms of the alms - in healpys ordering\n",
        "def hpalminit(_alms, _lmax):\n",
        "    _count = 0\n",
        "    for l in range(_lmax):\n",
        "        for m in range(l + 1):\n",
        "            _count = _count + 1\n",
        "            if _count == 1 or _count == 2 or _count == _lmax+1:\n",
        "                _alms[_count - 1] = complex(0,0)\n",
        "    _count = 0\n",
        "    for l in range(2*_lmax - 1):\n",
        "        _alms[_count] = complex(np.real(_alms[_count]),0)\n",
        "        _count = _count + 1\n",
        "    return _alms\n",
        "\n",
        "#matrix for the calculation of the psi in psi_tf\n",
        "def multtensor(_lmax,_lenalm):\n",
        "    _shape = np.zeros([_lmax,_lenalm])\n",
        "    _count = 0\n",
        "    for i in range(_lmax):\n",
        "        for j in np.arange(0,i+1):\n",
        "            if j == 0:\n",
        "                _shape[i][_count] = 1.0\n",
        "                _count = _count + 1\n",
        "            else:\n",
        "                _shape[i][_count] = 2.0\n",
        "                _count = _count + 1\n",
        "    return tf.convert_to_tensor(_shape, dtype = np.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#negative log of the posterior, psi.\n",
        "def psi(_params, _map, _lmax, _NSIDE, _Ninv): #unnormalised log probability\n",
        "    _lncl, _realalm, _imagalm = [0,0], [], []\n",
        "    for i in range(len_cl-2):\n",
        "        _lncl.append(_params[i])\n",
        "    for i in range(len_ralm):\n",
        "        _realalm.append(_params[i + len_cl-2])\n",
        "    for i in range(len_ialm-(2*lmax-1)):\n",
        "        _imagalm.append(_params[i + len_cl-2 + len_ralm])\n",
        "\n",
        "    _d = _map\n",
        "    _a = splittosingularalm(_realalm, _imagalm)\n",
        "    _Ya = hpalmtomap(almmotho(_a,_lmax), _NSIDE, _lmax)\n",
        "    _BYa =  _Ya #mapsmooth(_Ya, _lmax)\n",
        "\n",
        "    _elem, _term1, _term2, _psi1 ,_psi2, _psi3 = [], [], [], [], [], []\n",
        "    _sum = 0\n",
        "\n",
        "    for i in range(len(_d)):\n",
        "        _elem.append(_d[i] - _BYa[i])\n",
        "        _psi1.append(0.5*(_elem[i]**2)*_Ninv[i]) #first term in the taylor paper\n",
        "\n",
        "    _l = np.arange(lmax)\n",
        "    for i in range(len(_lncl)):\n",
        "        _psi2.append((_l[i] + 0.5)*(_lncl[i])) #second term in the taylor paper\n",
        "\n",
        "    _a = np.absolute(np.array(_a))**2\n",
        "    _as = np.matmul(shape.numpy(),_a)\n",
        "    _psi3 = 0.5*_as/np.exp(np.array(_lncl)) #third term in the taylor paper\n",
        "\n",
        "    _psi = sum(_psi1) + sum(_psi2) + sum(_psi3)\n",
        "    print('psi =',_psi)\n",
        "    return _psi\n",
        "\n",
        "\n",
        "#negative log of the posterior, psi.def psi(_params, _map, _lmax, _NSIDE, _Ninv): #unnormalised log probability    _lncl, _realalm, _imagalm = [0,0], [], []    for i in range(len_cl-2):        _lncl.append(_params[i])    for i in range(len_ralm):        _realalm.append(_params[i + len_cl-2])    for i in range(len_ialm-(2*lmax-1)):        _imagalm.append(_params[i + len_cl-2 + len_ralm])     _d = _map    _a = splittosingularalm(_realalm, _imagalm)    _Ya = hpalmtomap(almmotho(_a,_lmax), _NSIDE, _lmax)    _BYa =  _Ya #mapsmooth(_Ya, _lmax)        _elem, _term1, _term2, _psi1 ,_psi2, _psi3 = [], [], [], [], [], []    _sum = 0        for i in range(len(_d)):        _elem.append(_d[i] - _BYa[i])        _psi1.append(0.5*(_elem[i]**2)*_Ninv[i]) #first term in the taylor paper         _l = np.arange(lmax)    for i in range(len(_lncl)):        _psi2.append((_l[i] + 0.5)*(_lncl[i])) #second term in the taylor paper      _a = np.absolute(np.array(_a))**2    _as = np.matmul(shape.numpy(),_a)    _psi3 = 0.5*_as/np.exp(np.array(_lncl)) #third term in the taylor paper      _psi = sum(_psi1) + sum(_psi2) + sum(_psi3)     print('psi =',_psi)    return _psi\n",
        "def psi_tf(_params):\n",
        "    _map, _lmax, _NSIDE, _Ninv = noisemap_tf, lmax, NSIDE, Ninv\n",
        "    _lnclstart = tf.zeros(2, np.float64)\n",
        "    _lncl = tf.concat([_lnclstart,_params[:(len_cl - 2)]], axis = 0)\n",
        "    _realalm = _params[len_cl - 2:(len_ralm + len_cl - 2)]\n",
        "    _imagalm = _params[(len_ralm + len_cl - 2):]\n",
        "\n",
        "    _d = _map\n",
        "    _a = splittosingularalm_tf(_realalm, _imagalm)\n",
        "    _Ya = almtomap_tf(_a, _NSIDE, _lmax)\n",
        "    _BYa =  _Ya #mapsmooth(_Ya, _lmax)\n",
        "\n",
        "    _elem = _d - _BYa\n",
        "    _psi1 = 0.5*(_elem**2)*_Ninv #first term in the taylor paper\n",
        "\n",
        "    _l = tf.range(_lmax, dtype = np.float64)\n",
        "    _psi2 = (_l+0.5)*_lncl #second term in the taylor paper\n",
        "\n",
        "    _a = tf.math.abs(_a)**2\n",
        "    _as = tf.linalg.matvec(shape,_a)\n",
        "    _psi3 = 0.5*_as/tf.math.exp(_lncl) #third term in the taylor paper\n",
        "\n",
        "    _psi = tf.reduce_sum(_psi1) + tf.reduce_sum(_psi2) + tf.reduce_sum(_psi3)\n",
        "    #print(_psi)\n",
        "    __psi_record.append(_psi)\n",
        "    #print('psi1',tf.reduce_sum(_psi1),'psi2',tf.reduce_sum(_psi2),'psi3',tf.reduce_sum(_psi3))\n",
        "    return _psi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Run the normal hmc sampler\n",
        "def run_chain_hmc(modelparams, initial_state,_step_size = 0.01, num_results = 1000, num_burnin_steps=0, _n_lfs = 2):\n",
        "    '''Returns the desired walks through parameter space for a fixed step size.'''\n",
        "    hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(target_log_prob_fn=modelparams.psi_tf, step_size=_step_size, num_leapfrog_steps=_n_lfs)\n",
        "    return tfp.mcmc.sample_chain(num_results=num_results, num_burnin_steps=num_burnin_steps,\n",
        "                               current_state=initial_state, kernel=hmc_kernel, trace_fn=lambda current_state,\n",
        "                               kernel_results: kernel_results)\n",
        "#Run the nut sampler chain\n",
        "def run_chain_nut(modelparams, initial_state, _step_size, num_results=1000, num_burnin_steps=0, mtd = 10, med = 1000, u_lfs = 1, pi = 10):\n",
        "    '''Returns the desired walks through parameter space for a fixed step size.'''\n",
        "    nut_kernel = tfp.mcmc.NoUTurnSampler(target_log_prob_fn=modelparams.psi_tf, step_size=_step_size, max_tree_depth=mtd, max_energy_diff=med,\n",
        "                                         unrolled_leapfrog_steps=u_lfs, parallel_iterations=pi)\n",
        "    return tfp.mcmc.sample_chain(num_results=num_results, num_burnin_steps=num_burnin_steps,\n",
        "                               current_state=initial_state, kernel=nut_kernel, trace_fn=lambda current_state,\n",
        "                               kernel_results: kernel_results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMSm171R2fkJay5Li9e7E3u",
      "name": "Main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "healpy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
